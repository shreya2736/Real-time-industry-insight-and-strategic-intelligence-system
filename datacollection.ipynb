{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# installing the required libraries and packages\n",
        "!pip install newsapi-python google-search-results pandas numpy requests tweepy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o74maLgnP2sr",
        "outputId": "cc4e5015-3984-42c2-fa7f-24bc164f3a66"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: newsapi-python in /usr/local/lib/python3.12/dist-packages (0.2.7)\n",
            "Requirement already satisfied: google-search-results in /usr/local/lib/python3.12/dist-packages (2.4.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: tweepy in /usr/local/lib/python3.12/dist-packages (4.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.8.3)\n",
            "Requirement already satisfied: oauthlib<4,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from tweepy) (3.3.1)\n",
            "Requirement already satisfied: requests-oauthlib<3,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from tweepy) (2.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "R3aCr8n1Pjmz"
      },
      "outputs": [],
      "source": [
        "# importing the required modules\n",
        "from google.colab import userdata\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from serpapi import GoogleSearch\n",
        "import tweepy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================== SETTING UP THE CONFIGURATION =====================\n",
        "\n",
        "# search keywords\n",
        "QUERY = \"industry trends OR competitor analysis OR market insights OR Artificial Intelligence\"\n",
        "MAX_RESULTS = 100  # to fetch 100 items per source\n",
        "NEWSAPI_KEY = userdata.get(\"NEWS_API_KEY\")           # From https://newsapi.org\n",
        "SERPAPI_KEY = userdata.get(\"SERP_API_KEY\")            # From https://serpapi.com\n",
        "TWITTER_BEARERTOKEN = userdata.get(\"TWITTER_BEARERTOKEN\" ) # From https://developer.x.com # as twitter has limits i am going to fetch only 30 tweets which are relevant"
      ],
      "metadata": {
        "id": "FpxFF410QGB_"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================== COLLECTING DATA FROM NEWSAPI =====================\n",
        "def fetch_newsapi(query):\n",
        "    url = f\"https://newsapi.org/v2/everything?q={query}&pageSize={MAX_RESULTS}&apiKey={NEWSAPI_KEY}\"\n",
        "\n",
        "    # Send an HTTP GET request to the NewsAPI endpoint\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Check if the request was successful (status code 200 = OK)\n",
        "    if response.status_code != 200:\n",
        "        print(\"NewsAPI Error:\", response.text)\n",
        "        return []\n",
        "\n",
        "    # Extract the list of articles from the JSON response\n",
        "    articles = response.json().get(\"articles\", [])\n",
        "    news_data = []\n",
        "\n",
        "     # Loop through each article returned by the API\n",
        "    for art in articles:\n",
        "        news_data.append({\n",
        "            \"title\": art.get(\"title\"),\n",
        "            \"description\": art.get(\"description\"),\n",
        "            \"url\": art.get(\"url\"),\n",
        "            \"publishedAt\": art.get(\"publishedAt\"),\n",
        "            \"source\": art.get(\"source\", {}).get(\"name\"),\n",
        "            \"type\": \"news\"\n",
        "        })\n",
        "    return news_data"
      ],
      "metadata": {
        "id": "j9KFu8VBQk-c"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================== COLLECTING DATA FROM SERPAPI =====================\n",
        "def fetch_serpapi(query):\n",
        "    # Creating a GoogleSearch object from SerpAPI with the following parameters:\n",
        "    # \"q\": The search query (keywords for news)\n",
        "    # \"api_key\": Your SerpAPI key for authentication\n",
        "    # \"tbm\": \"nws\" specifies that we want to search only in Google News\n",
        "    # \"num\": The maximum number of results to fetch (set to MAX_RESULTS)\n",
        "    search = GoogleSearch({\n",
        "        \"q\": query,\n",
        "        \"api_key\": SERPAPI_KEY,\n",
        "        \"tbm\": \"nws\",  # news search\n",
        "        \"num\": MAX_RESULTS\n",
        "    })\n",
        "\n",
        "    # Convert the search results into a dictionary and extract only the 'news_results' field.\n",
        "    results = search.get_dict().get(\"news_results\", [])\n",
        "    serp_data = []\n",
        "    for item in results:\n",
        "        serp_data.append({\n",
        "            \"title\": item.get(\"title\"),\n",
        "            \"description\": item.get(\"snippet\"),\n",
        "            \"url\": item.get(\"link\"),\n",
        "            \"publishedAt\": item.get(\"date\"),\n",
        "            \"source\": item.get(\"source\"),\n",
        "            \"type\": \"news\"\n",
        "        })\n",
        "    return serp_data"
      ],
      "metadata": {
        "id": "cKcokoGSQpom"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================== COLLECTING DATA FROM TWITTER =====================\n",
        "def fetch_twitter(query):\n",
        "\n",
        "  # Create a Twitter API client using Tweepy with the Bearer Token\n",
        "    client = tweepy.Client(bearer_token=TWITTER_BEARERTOKEN)\n",
        "\n",
        "    # Search for recent tweets matching the query\n",
        "    # query: search keywords or hashtags\n",
        "    # tweet_fields: specify what fields to retrieve (created_at = timestamp, author_id = user)\n",
        "    # max_results: maximum number of tweets to retrieve (limited to 100 per request)\n",
        "    tweets = client.search_recent_tweets(\n",
        "        query=query, tweet_fields=[\"created_at\", \"author_id\"], max_results=30\n",
        "    )\n",
        "    tweet_data = []\n",
        "    if tweets.data:\n",
        "        for tweet in tweets.data:\n",
        "            tweet_data.append({\n",
        "                \"title\": tweet.text[:70] + \"...\",  # Short preview\n",
        "                \"description\": tweet.text,\n",
        "                \"url\": f\"https://twitter.com/i/web/status/{tweet.id}\",\n",
        "                \"publishedAt\": tweet.created_at,\n",
        "                \"source\": \"Twitter\",\n",
        "                \"type\": \"tweet\"\n",
        "            })\n",
        "    return tweet_data"
      ],
      "metadata": {
        "id": "fYreVmfUQv12"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================== DATA COLLECTION =====================\n",
        "def collect_all_data(query):\n",
        "    newsapi_data = fetch_newsapi(query) # Fetch articles from NewsAPI\n",
        "    serpapi_data = fetch_serpapi(query) # Fetch articles from Google SERP API\n",
        "    twitter_data = fetch_twitter(query) # Fetch tweets from Twitter API\n",
        "\n",
        "\n",
        "    # Combine only non-empty lists\n",
        "    combined_data = []\n",
        "    if newsapi_data:\n",
        "        combined_data.extend(newsapi_data)\n",
        "    if serpapi_data:\n",
        "        combined_data.extend(serpapi_data)\n",
        "    if twitter_data:\n",
        "        combined_data.extend(twitter_data)\n",
        "\n",
        "    if not combined_data:\n",
        "        print(\"No data collected!\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    df = pd.DataFrame(combined_data)\n",
        "\n",
        "    # Ensure that the type column is clean\n",
        "    df['type'] = df['type'].str.lower().fillna(\"news\")\n",
        "    df.loc[df['url'].str.contains(\"twitter.com\", case=False, na=False), 'type'] = \"tweet\"\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "O-z1tsK2Qzn1"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================== RUN SCRIPT =====================\n",
        "df = collect_all_data(QUERY)\n",
        "if not df.empty:\n",
        "    output_path = \"industry_insights_clean.csv\"\n",
        "    df.to_csv(output_path, index=False)\n",
        "    print(f\"Data collected and saved to: {output_path}\")\n",
        "else:\n",
        "    print(\"No data to save.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epOgF0GOQ3Lb",
        "outputId": "880ed0f8-ce1c-448f-a6c7-0c0534e7dc59"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data collected and saved to: industry_insights_clean.csv\n"
          ]
        }
      ]
    }
  ]
}